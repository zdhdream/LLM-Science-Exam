{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "import wikipediaapi\n",
    "import itables\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify OpenAI API key in Kaggle's secrets add-ons.\n",
    "\n",
    "\n",
    "openai.api_base = \"https://api.chatanywhere.com.cn/v1\"\n",
    "openai.api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/nbroad/create-science-wikipedia-dataset\n",
    "files = list(map(str, Path(\"./data/wiki-20220301-en-sci/\").glob(\"*.parquet\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-75da5cf647e8e8ea\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/default to C:/Users/ROG/.cache/huggingface/datasets/parquet/default-75da5cf647e8e8ea/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e932166683154eb3a5700ac411aaae72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe6dbba15414c79bcd2064fa8bfb38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e449cbcbfeca4249b84bf6eb6703ad0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to C:/Users/ROG/.cache/huggingface/datasets/parquet/default-75da5cf647e8e8ea/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"parquet\", data_files=files, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "options_set = set((\"option_1\", \"option_2\", \"option_3\", \"option_4\", \"option_5\"))\n",
    "response_keys_set = set((\"question\", \"option_1\", \"option_2\", \"option_3\", \"option_4\", \"option_5\", \"answer\"))\n",
    "\n",
    "delimiter = \"####\"\n",
    "system_message = f\"\"\"\n",
    "You will be provided with TEXT from wikipedia. \\\n",
    "The TEXT will be delimited with {delimiter} characters.\n",
    "Output a python list of 5 dict objects, where each object is \\\n",
    "a multiple choice question whom answers should be in \\\n",
    "the given TEXT and that has 5 choices each and has the following format:\n",
    "    'question': <question on the TEXT>\n",
    "    'option_1': <question answer option>\n",
    "    'option_2': <question answer option>\n",
    "    'option_3': <question answer option>\n",
    "    'option_4': <question answer option>\n",
    "    'option_5': <question answer option>\n",
    "    'answer': <answer option key label>\n",
    "\n",
    "You should tell me which one of your proposed options is right \\\n",
    "by assigning the corresponding option's key label in the 'answer' field.\n",
    "\n",
    "The question, the answer and question answer options should be broad, \\\n",
    "challenging, long, detailed and based on the TEXT provided.\n",
    "\n",
    "Only output the list of objects, with nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion_messages(wiki_text):\n",
    "    return [  \n",
    "        {\n",
    "            'role':'system', \n",
    "            'content': system_message\n",
    "        },    \n",
    "        {\n",
    "            'role':'user', \n",
    "            'content': f\"{delimiter}{wiki_text}{delimiter}\"\n",
    "        },  \n",
    "    ]\n",
    "\n",
    "def get_completion_from_messages(\n",
    "    messages, \n",
    "    model=\"gpt-3.5-turbo-16k\", \n",
    "    temperature=0.8, \n",
    "    max_tokens=6000\n",
    "):\n",
    "    try:\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "\n",
    "            model= model,\n",
    "\n",
    "            messages=messages,\n",
    "            temperature = temperature,\n",
    "            max_tokens = max_tokens,\n",
    "            stream=True,\n",
    "\n",
    "        )\n",
    "\n",
    "        completion = {'role': '', 'content': ''}\n",
    "\n",
    "        for event in response:\n",
    "\n",
    "            if event['choices'][0]['finish_reason'] == 'stop':\n",
    "\n",
    "                #print(f'收到的完成数据: {completion}')\n",
    "\n",
    "                break\n",
    "\n",
    "            for delta_k, delta_v in event['choices'][0]['delta'].items():\n",
    "\n",
    "                #print(f'流响应数据: {delta_k} = {delta_v}')\n",
    "\n",
    "                completion[delta_k] += delta_v\n",
    "\n",
    "        messages.append(completion)  # 直接在传入参数 messages 中追加消息\n",
    "\n",
    "        return (True, '')\n",
    "\n",
    "    except Exception as err:\n",
    "\n",
    "        return (False, f'OpenAI API 异常: {err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_correctly_formatted(mcq) -> bool:\n",
    "    return all([\n",
    "        len(el) == len(response_keys_set) and response_keys_set == set(list(el.keys()))\n",
    "        for el in mcq\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/25000 [00:31<108:33:03, 15.63s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_completion_attempts = 5\n",
    "multiple_choice_questions = []\n",
    "\n",
    "for index, row in tqdm(enumerate(ds), total=len(ds)):\n",
    "   \n",
    "    text = row['text']\n",
    "    message = get_completion_messages(text)    \n",
    "\n",
    "\n",
    "    attempts_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            get_completion_from_messages(message)\n",
    "            assert len(message) == 3\n",
    "            chatgpt_response = message[-1]['content']\n",
    "            #print(chatgpt_response)\n",
    "            mcq = eval(chatgpt_response)\n",
    "\n",
    "            if not isinstance(mcq, list) or len(mcq) < 5 or not is_correctly_formatted(mcq):\n",
    "                raise Exception\n",
    "\n",
    "            for i in range(len(mcq)):\n",
    "                mcq[i][\"ori_index\"] = index\n",
    "                mcq[i][\"ori_wiki\"] = text\n",
    "                if mcq[i][\"answer\"] in options_set:\n",
    "                    continue\n",
    "                else:\n",
    "                    # index method will raise an error if answer isn't in list\n",
    "                    answ_indx = [v.lower() for v in mcq[i].values()].index(mcq[i][\"answer\"].lower())\n",
    "                    mcq[i][\"answer\"] = list(mcq[i].keys())[answ_indx]\n",
    "\n",
    "            multiple_choice_questions += mcq\n",
    "            #print(\"Generated count:\", index+1)\n",
    "            if len(multiple_choice_questions) % 5 == 0:\n",
    "                df_mcqcheck = pd.DataFrame.from_records(multiple_choice_questions)\n",
    "                df_mcqcheck.to_csv(str(index+1)+\"-check_ds.csv\", index=None)\n",
    "            break\n",
    "        except Exception:\n",
    "            attempts_counter += 1\n",
    "            print(\"Attempts count:\", attempts_counter)\n",
    "            if attempts_counter > max_completion_attempts:\n",
    "                break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(multiple_choice_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mcq = pd.DataFrame.from_records(multiple_choice_questions)\n",
    "df_mcq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcq.to_csv('train_qa.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ori_df = pd.DataFrame(ds)\n",
    "ori_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_df.to_csv('ori_dataset.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_final = df_mcq.merge(ori_df, how='left', left_on='ori_text', right_on= 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final[['question', 'text', 'url', 'title']]\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"retrive_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv(\"retrive_dataset.csv\")\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg= []\n",
    "for row in tqdm(df_final.iterrows(), total=len(df_final)):\n",
    "    row = row[1]\n",
    "    url = row.url\n",
    "    question = row.question\n",
    "    candidates = ori_df[ori_df['url'] != url]\n",
    "    candidates = candidates.sample(n=10)\n",
    "    candidates['ori_url'] = url\n",
    "    candidates['question'] = question\n",
    "    neg.append(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = pd.concat(neg)\n",
    "neg = neg.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg.to_csv('neg.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neg = pd.read_csv('neg.csv')\n",
    "neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = pd.read_csv('neg.csv')\n",
    "neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neg = pd.read_csv('neg.csv')\n",
    "neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_id = ori_df.url.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_id = val_id[:1000]\n",
    "np.save('val_id',val_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
